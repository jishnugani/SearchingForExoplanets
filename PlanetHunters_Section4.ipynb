{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF-sZBxg8P-9"
      },
      "source": [
        "<font color=orange><h5><b>[REMINDER: MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT]</b></h1></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bs0JRv4DSs06",
        "outputId": "21c6e82b-1b46-4b77-c656-3da001138300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "#@title Instructor Tokens\n",
        "#@markdown Use these sparingly, as only one person can use each at a time!\n",
        "\n",
        "!ngrok authtoken 2esTEyBVYVUkNTh0ETgpZq6ThNH_RzyMevymuyYSKd76mSY7\n",
        "# !ngrok authtoken 2esTQObOz9XZ2be2DF4aPlS3upu_88X3yovgR3x3NFNAVTvVf\n",
        "# !ngrok authtoken 2PfMYFZgmz9KLqLlZ4Pn7Bqa8ZA_3vMo1vufxRTWJLHhDHUBC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PedG4sB4gk7v"
      },
      "source": [
        "# **ML Deployment: Planet Hunters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2YbbXcdgMyE"
      },
      "source": [
        "<img src=\"https://www.eso.org/public/archives/images/screen/eso2202a.jpg\" width=\"730\" height=\"450\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC50V7gvSp4V",
        "outputId": "8acb8bb0-7a6d-4ba2-e24c-1c03cd8752e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exoTrain.csv        100%[===================>] 250.08M   113MB/s    in 2.2s    \n",
            "exoTest.csv         100%[===================>]  27.57M   122MB/s    in 0.2s    \n"
          ]
        }
      ],
      "source": [
        "#@title Run this to setup libraries and environment\n",
        "import os\n",
        "import sys\n",
        "\n",
        "class HiddenPrints:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "\n",
        "with HiddenPrints():\n",
        "\n",
        "    # Installing Streamlit & pyngrok\n",
        "    !pip -q install streamlit\n",
        "    !pip -q install pyngrok\n",
        "    from pyngrok import ngrok\n",
        "    import streamlit as st\n",
        "    !wget \"https://raw.githubusercontent.com/NolanChai/model_repo/main/income.csv\"\n",
        "\n",
        "def launch_website():\n",
        "  print (\"Click this link to try your web app:\")\n",
        "  if (ngrok.get_tunnels() != None):\n",
        "    ngrok.kill()\n",
        "  public_url = ngrok.connect()\n",
        "  print (public_url)\n",
        "  !streamlit run --server.port 80 app.py >/dev/null\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "from pathlib import Path\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTrain.csv'\n",
        "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTest.csv'\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import  metrics\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score,ConfusionMatrixDisplay,precision_score,recall_score,f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, normalize\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv1D, Conv2D, MaxPooling2D, BatchNormalization, MaxPooling1D\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "df_train = pd.read_csv('exoTrain.csv')\n",
        "df_train['LABEL'] = df_train['LABEL'] -1\n",
        "df_test = pd.read_csv('exoTest.csv')\n",
        "df_test['LABEL'] = df_test['LABEL'] - 1\n",
        "\n",
        "def plot_graphs(history, best):\n",
        "\n",
        "  plt.figure(figsize=[10,4])\n",
        "  # summarize history for accuracy\n",
        "  plt.subplot(121)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy across training\\n best accuracy of %.02f'%best[1])\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "  # summarize history for loss\n",
        "  plt.subplot(122)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss across training\\n best loss of %.02f'%best[0])\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def analyze_results(model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Helper function to help interpret and model performance.\n",
        "\n",
        "    Args:\n",
        "    model: estimator instance\n",
        "    X_train: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "    Input values for model training.\n",
        "    y_train : array-like of shape (n_samples,)\n",
        "    Target values for model training.\n",
        "    X_test: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "    Input values for model testing.\n",
        "    y_test : array-like of shape (n_samples,)\n",
        "    Target values for model testing.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    print(\"-------------------------------------------\")\n",
        "    print(\"Model Results\")\n",
        "    print(\"\")\n",
        "    print(\"Training:\")\n",
        "    if type(model) == keras.src.engine.sequential.Sequential:\n",
        "      train_predictions = model.predict(X_train)\n",
        "      train_predictions = (train_predictions > 0.5)\n",
        "      cm = confusion_matrix(y_train, train_predictions)\n",
        "      labels = [0, 1]\n",
        "      df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
        "      fig = plt.figure()\n",
        "      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "      #plt.yticks([1.25, 3.75], labels,va='center')\n",
        "      plt.title('Confusion Matrix - Test Data')\n",
        "      plt.ylabel('True label')\n",
        "      plt.xlabel('Predicted label')\n",
        "      plt.show()\n",
        "    else:\n",
        "      plt.close()\n",
        "      ConfusionMatrixDisplay.from_estimator(model,X_train,y_train)\n",
        "      plt.show()\n",
        "\n",
        "    print(\"Testing:\")\n",
        "    if type(model) == keras.src.engine.sequential.Sequential:\n",
        "      test_predictions = model.predict(X_test)\n",
        "      test_predictions = (test_predictions > 0.5)\n",
        "      cm = confusion_matrix(y_test, test_predictions)\n",
        "      labels = [0, 1]\n",
        "      df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
        "      fig = plt.figure()\n",
        "      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "      #plt.yticks([1.25, 3.75], labels,va='center')\n",
        "      plt.title('Confusion Matrix - Test Data')\n",
        "      plt.ylabel('True label')\n",
        "      plt.xlabel('Predicted label')\n",
        "      plt.show()\n",
        "    else:\n",
        "      ConfusionMatrixDisplay.from_estimator(model,X_test,y_test)\n",
        "\n",
        "X_train = df_train.drop('LABEL', axis=1)\n",
        "y_train = df_train['LABEL']\n",
        "X_test = df_test.drop('LABEL', axis=1)\n",
        "y_test = df_test['LABEL']\n",
        "\n",
        "# Preprocess Data\n",
        "# Helper functions that we can run for the three augmentation functions that will be used, but not explroed in depth\n",
        "\n",
        "def smote(a,b):\n",
        "    model = SMOTE()\n",
        "    X,y = model.fit_resample(a, b)\n",
        "    return X,y\n",
        "\n",
        "def savgol(df1,df2):\n",
        "    x = savgol_filter(df1,21,4,deriv=0)\n",
        "    y = savgol_filter(df2,21,4,deriv=0)\n",
        "    return x,y\n",
        "\n",
        "def fourier(df1,df2):\n",
        "    X_train = np.abs(np.fft.fft(df1, axis=1))\n",
        "    X_test = np.abs(np.fft.fft(df2, axis=1))\n",
        "    return X_train,X_test\n",
        "\n",
        "def norm(df1,df2):\n",
        "    X_train = normalize(df1)\n",
        "    X_test = normalize(df2)\n",
        "    return X_train,X_test\n",
        "\n",
        "def robust(df1,df2):\n",
        "    scaler = RobustScaler()\n",
        "    X_train = scaler.fit_transform(df1)\n",
        "    X_test = scaler.transform(df2)\n",
        "    return X_train,X_test\n",
        "\n",
        "fourier_X_train, fourier_X_test = fourier(X_train, X_test)\n",
        "savgol_X_train, savgol_X_test = savgol(fourier_X_train, fourier_X_test)\n",
        "norm_X_train, norm_X_test = norm(savgol_X_train,savgol_X_test)\n",
        "robust_X_train, robust_X_test = robust(norm_X_train, norm_X_test)\n",
        "smote_X_train,smote_y_train = smote(robust_X_train, y_train)\n",
        "\n",
        "# Here we're adding the generated, augmented data onto the testing data\n",
        "# aug_X_train, new_X_test_data, aug_y_train, new_y_test_data = train_test_split(smote_X_train, smote_y_train, test_size=0.3)\n",
        "# aug_X_test = np.concatenate((robust_X_test, new_X_test_data), axis=0)\n",
        "# aug_y_test = np.concatenate((y_test, new_y_test_data), axis=0)\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Apply preprocessing steps to the dataframe.\"\"\"\n",
        "    X = df.drop('LABEL', axis=1).values\n",
        "    y = df['LABEL'].values\n",
        "    # Fourier transform\n",
        "    X = np.abs(np.fft.fft(X, axis=1))\n",
        "    # Savitzky-Golay filter\n",
        "    X = savgol_filter(X, 21, 4, deriv=0, axis=1)\n",
        "    # Normalize\n",
        "    X = normalize(X)\n",
        "    # Robust scaling\n",
        "    scaler = RobustScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    # SMOTE\n",
        "    smote = SMOTE()\n",
        "    X, y = smote.fit_resample(X, y)\n",
        "    # Expand dimensions for CNN input\n",
        "    X_cnn = np.expand_dims(X, axis=2)\n",
        "    return X, X_cnn, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnlqyemLh_fh"
      },
      "source": [
        "Great work! We've been able to develop and compare various models to identify potential habitable planets from photometric data - now we can try deploying them to the web! While your site is running, you'll be able to use it on your laptop or computer or share it with friends! We'll be using [Streamlit](https://www.streamlit.io/), a library of website objects and methods that allows us to quickly build a site."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkVsPLnNXtSc"
      },
      "source": [
        "# **Part 1. Streamlit - Deploying your model to the web**\n",
        "\n",
        "The goal of this session is to learn how to deploy the models that we have been training to the web so they can be shared with the world!\n",
        "\n",
        "Let's start with an example.\n",
        "\n",
        "Check out [this example](https://bgremoval.streamlit.app/) and answer the following\n",
        "**questions:**\n",
        "* Who is this application for?\n",
        "* How does the user input data - are these intuitive ways of interacting with the app?\n",
        "* What does the application do with the data?\n",
        "* Evaluate the ease of use and look of the application.\n",
        "\n",
        "(More cool examples [here](https://streamlit.io/gallery?category=favorites)!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbe3BD0uio_o"
      },
      "source": [
        "Streamlit interprets Python files as a website! This is great for several reasons\n",
        "* No need to know HTML, CSS, Javascript,... etc\n",
        "* Easy to use our trained models which are already in Python!\n",
        "\n",
        "We'll write everything to a file called app.py, which is used for Streamlit to launch the site."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdOdJ-ODdahf"
      },
      "source": [
        "```\n",
        "%%writefile app.py\n",
        "```\n",
        "The `%%writefile` command writes to a file (and creates one if it doesn't already exist!). Everything that follows the rest of this block of code will be written to `app.py`. <br> In this case, we use this command to create our `app.py` file, which you can check in the 'Files' tab in left sidebar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-d_ZK4Jmjw8",
        "outputId": "ff7d9b23-3e4a-4cf7-f60b-98ffe94fdf53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "st.title('Hello World!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHaXdm2yne2G"
      },
      "source": [
        "To launch our `app.py`, we need to use a combination of Streamlit and ngrok. Ngrok is a server hosting service, and Streamlit is a library that allows us to connect our Python code to Ngrok. <br> To get access to our Ngrok server, we need to sign up on their website and get a unique **authentication token**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8gtYgrXjnjg"
      },
      "source": [
        "To launch our `app.py`, we need to use a combination of Streamlit and ngrok. Ngrok serves as a hosting service that enables us to establish secure tunnels to our localhost, making our local server accessible over the internet. Streamlit, on the other hand, is a powerful library designed to turn Python scripts into interactive web applications easily. Together, Ngrok allows us to host our Streamlit code publicly on the web!\n",
        "\n",
        "<br> To get access to our Ngrok server, we need to sign up on their website and get a unique **authentication token**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxdAMUKRn5mj"
      },
      "source": [
        "<font color=SlateGrey><h2><b>\n",
        "Use [these](https://drive.google.com/file/d/12zwuOuKh91VSHIHS-6S4ADF4HLC2wKJq/view?usp=sharing) instructions to create a ngrok account and get your authtoken!\n",
        "</b></h2></font>\n",
        "\n",
        "<font color=DarkGray><h3><b>\n",
        "Paste your authtoken below next to `!ngrok authtoken`!\n",
        "</b></h3></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDs-_mOUm1DB",
        "outputId": "2e328df4-823d-4967-c408-14e12fee3962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2jqZZ5QOAyg7ndiavkl74putT7V_tiXgYZ24dz1UkRB9S7uD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTuX65aQpS3Z"
      },
      "source": [
        "Now, we can launch our website through the `launch_website()` function, which connects our ngrok token by building a 'tunnel' to our Streamlit code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUwUJ7Mtq8eD"
      },
      "source": [
        "We can use the following launch_website() function to deploy our `app.py` file. Click on the first link, and hit the `Visit Site` button to view your site!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNTZdEYBiWb4",
        "outputId": "8b4b1081-4ff0-448c-a745-23ba7556f378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click this link to try your web app:\n",
            "NgrokTunnel: \"https://325b-35-185-1-182.ngrok-free.app\" -> \"http://localhost:80\"\n"
          ]
        }
      ],
      "source": [
        "launch_website()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hzPortU1qyv"
      },
      "source": [
        "<font color=Blue><b>\n",
        "Congratulations, you have your first working site!\n",
        "</b></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLr1ITQlrMuS"
      },
      "source": [
        "Now the question remains, how do we connect our models and data to our website?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21apfMGTnquH"
      },
      "source": [
        "## **Step 2: Loading the model**\n",
        "\n",
        "Let's load our previously trained model! :)\n",
        "\n",
        "Here is a reference on how to save and load sklearn and tensorflow models!\n",
        "\n",
        "For sklearn:\n",
        "```\n",
        "from joblib import dump, load\n",
        "\n",
        "# ====== Save model ========\n",
        "dump(model, 'filename.joblib')\n",
        "\n",
        "# ====== Load model ========\n",
        "clf = load('filename.joblib')\n",
        "```\n",
        "\n",
        "For tensorflow:\n",
        "```\n",
        "import tensorflow as tf\n",
        "\n",
        "# ====== Save model ========\n",
        "model.save(\"filename.h5\")\n",
        "\n",
        "# ====== Load model ========\n",
        "tf.keras.models.load_model(\"filename.h5\")\n",
        "```\n",
        "\n",
        "The CNN model we worked on last time is a tensorflow model! Let's load in the model we created last time:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOq7pgUqhxiv",
        "outputId": "afc523a4-bf18-498b-a149-19837a4d53da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 3197, 8)           48        \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 800, 8)            0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 800, 16)           400       \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 200, 16)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3200)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 3201      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3649 (14.25 KB)\n",
            "Trainable params: 3649 (14.25 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to load the model\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "cnn_path = '/content/gdrive/My Drive/cnn.zip'\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(cnn_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('')\n",
        "model = tf.keras.models.load_model('cnn')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mqpq-bGjx4I_"
      },
      "source": [
        "And preprocess our data before loading it into our site:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "QC0RjNOLDnNA"
      },
      "outputs": [],
      "source": [
        "X_train, X_train_cnn, y_train = preprocess_data(df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRTwhXqOyJiK"
      },
      "source": [
        "Recall a few of the visualizations we've done so far:\n",
        "* Light Curve Graphs\n",
        "* Confusion Matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlECAQeV-o5X"
      },
      "source": [
        "## **Step 3: Graphs**\n",
        "Now that we have our model imported, we need to consider a few ways the user might be able to interact with it and the results that we've found thus far! One of the best (and easiest) ways to do this is through graphs. Streamlit works best with plotly graphs, a similar yet more visually appealing library of graph functions to matplotlib.\n",
        "\n",
        "### Revisiting Light Curve Graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXLxud1j7Wc2"
      },
      "source": [
        "We've gone ahead and imported the light curve visualizations from our past notebooks into a Streamlit site.\n",
        "\n",
        "**After changing the filename to your saved model's path,** go ahead and give it a try!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbnz5SFPmif1",
        "outputId": "8f178ca9-2f82-4002-a45f-eb5b3e0d2bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "################################################################################\n",
        "##########=           (Setup Code from Previous NBs)           =################\n",
        "################################################################################\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Load data\n",
        "df_train = pd.read_csv('exoTrain.csv')\n",
        "df_train['LABEL'] = df_train['LABEL'] - 1\n",
        "\n",
        "# Load the CNN model\n",
        "model = tf.keras.models.load_model('cnn')\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Apply preprocessing steps to the dataframe.\"\"\"\n",
        "    X = df.drop('LABEL', axis=1).values\n",
        "    y = df['LABEL'].values\n",
        "    # Fourier transform\n",
        "    X = np.abs(np.fft.fft(X, axis=1))\n",
        "    # Savitzky-Golay filter\n",
        "    X = savgol_filter(X, 21, 4, deriv=0, axis=1)\n",
        "    # Normalize\n",
        "    X = normalize(X)\n",
        "    # Robust scaling\n",
        "    scaler = RobustScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    # SMOTE\n",
        "    smote = SMOTE()\n",
        "    X, y = smote.fit_resample(X, y)\n",
        "    # Expand dimensions for CNN input\n",
        "    X = np.expand_dims(X, axis=2)\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = preprocess_data(df_train)\n",
        "\n",
        "def predict(index):\n",
        "    \"\"\"Make a prediction using the Keras model.\"\"\"\n",
        "    tensor = X_train[index].reshape(1, -1, 1)\n",
        "    output = model.predict(tensor)\n",
        "    return output.flatten()[0]\n",
        "\n",
        "################################################################################\n",
        "##########=                   (Light Curves)                   =################\n",
        "################################################################################\n",
        "st.title('Exoplanet Light Curve Visualization with CNN Predictions')\n",
        "\n",
        "# Slider for selecting the index of the light curve\n",
        "index = st.slider(\"Select Index for Light Curve\", min_value=0, max_value=len(X_train)-1, value=12, step=1)\n",
        "\n",
        "# Display CNN prediction results\n",
        "prediction = predict(index)\n",
        "st.write(f\"Prediction (probability of being an exoplanet): {prediction:.4f}\")\n",
        "t_0 = st.slider(\"Start of Period (t_0)\", min_value=0, max_value=3197, value=430, step=1)\n",
        "period = st.slider(\"Length of Period\", min_value=0, max_value=3197, value=1184, step=1)\n",
        "\n",
        "# Create Plotly graph for the full light curve with a rectangle highlighting the period\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(y=X_train[index].flatten(), mode='lines', name='Light Curve'))\n",
        "fig.add_shape(type=\"rect\",\n",
        "              x0=t_0, y0=min(X_train[index])-5,\n",
        "              x1=t_0+period, y1=max(X_train[index])+5,\n",
        "              line=dict(color=\"Red\"),\n",
        "              fillcolor=\"LightPink\", opacity=0.5)\n",
        "fig.update_layout(title=\"Box Covering One Period of Exoplanet Transit\",\n",
        "                  xaxis_title=\"Observation Point\",\n",
        "                  yaxis_title=\"Normalized Flux\",\n",
        "                  showlegend=True)\n",
        "st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "# Create Plotly graph for just the selected period\n",
        "fig_period = go.Figure()\n",
        "fig_period.add_trace(go.Scatter(y=X_train[index, t_0: t_0+period].flatten(), mode='lines', name='Selected Period'))\n",
        "fig_period.update_layout(title=\"Plot of Just One Period\",\n",
        "                         xaxis_title=\"Observation Point\",\n",
        "                         yaxis_title=\"Normalized Flux\",\n",
        "                         showlegend=True)\n",
        "st.plotly_chart(fig_period, use_container_width=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX-xCHFs4CVk"
      },
      "source": [
        "**Launch your website again:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oenZdmDoCma",
        "outputId": "30174931-097f-4c35-bf40-6c926b4813f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click this link to try your web app:\n",
            "NgrokTunnel: \"https://060c-35-185-1-182.ngrok-free.app\" -> \"http://localhost:80\"\n"
          ]
        }
      ],
      "source": [
        "launch_website()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJHQKJqk9Wb0"
      },
      "source": [
        "We can do the same thing for our confusion matrices as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAnsgktS9V5v",
        "outputId": "8b706504-14b3-4cf9-bc11-05de15e46be1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "model_path = 'Planet_Hunters_CNN.h5'\n",
        "################################################################################\n",
        "##########=           (Setup Code from Previous NBs)           =################\n",
        "################################################################################\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Load data\n",
        "df_train = pd.read_csv('exoTrain.csv')\n",
        "df_train['LABEL'] = df_train['LABEL'] - 1\n",
        "\n",
        "# Load the CNN model\n",
        "model = tf.keras.models.load_model('cnn')\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Apply preprocessing steps to the dataframe.\"\"\"\n",
        "    X = df.drop('LABEL', axis=1).values\n",
        "    y = df['LABEL'].values\n",
        "    # Fourier transform\n",
        "    X = np.abs(np.fft.fft(X, axis=1))\n",
        "    # Savitzky-Golay filter\n",
        "    X = savgol_filter(X, 21, 4, deriv=0, axis=1)\n",
        "    # Normalize\n",
        "    X = normalize(X)\n",
        "    # Robust scaling\n",
        "    scaler = RobustScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    # SMOTE\n",
        "    smote = SMOTE()\n",
        "    X, y = smote.fit_resample(X, y)\n",
        "    # Expand dimensions for CNN input\n",
        "    X = np.expand_dims(X, axis=2)\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = preprocess_data(df_train)\n",
        "\n",
        "def predict(index):\n",
        "    \"\"\"Make a prediction using the Keras model.\"\"\"\n",
        "    tensor = X_train[index].reshape(1, -1, 1)\n",
        "    output = model.predict(tensor)\n",
        "    return output.flatten()[0]\n",
        "\n",
        "################################################################################\n",
        "##########=                   (Confusion Matrix)               =################\n",
        "################################################################################\n",
        "st.title('Exoplanet Light Curve Visualization with CNN Predictions')\n",
        "\n",
        "# Slider for selecting the index of the light curve\n",
        "index = st.slider(\"Select Index for Light Curve\", min_value=0, max_value=len(X_train)-1, value=12, step=1)\n",
        "\n",
        "# Display CNN prediction results\n",
        "prediction = predict(index)\n",
        "st.write(f\"Prediction (probability of being an exoplanet): {prediction:.4f}\")\n",
        "\n",
        "# Compute and plot confusion matrix\n",
        "cnn_pred_labels = (model.predict(X_train).flatten() > 0.5).astype(int)\n",
        "cm_cnn = confusion_matrix(y_train, cnn_pred_labels)\n",
        "\n",
        "# Confusion Matrix for CNN\n",
        "fig_cm_cnn = ff.create_annotated_heatmap(z=cm_cnn, x=['Pred 0', 'Pred 1'], y=['True 0', 'True 1'],\n",
        "                                         colorscale='Viridis', showscale=True)\n",
        "fig_cm_cnn.update_layout(title='Confusion Matrix for CNN')\n",
        "st.plotly_chart(fig_cm_cnn, use_container_width=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqZWG2JD-z95"
      },
      "source": [
        "**Launch your website again:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "racBfSk1-z95",
        "outputId": "1ba0638e-0ba6-4de4-eefb-a0d019c2e76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click this link to try your web app:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2024-07-27T21:39:26+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/tunnels/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2024-07-27T21:39:26+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/tunnels/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2024-07-27T21:39:26+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/tunnels/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2024-07-27T21:39:26+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/tunnels/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/tunnels/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-51617c9bbb4d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlaunch_website\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-c54cde05af29>\u001b[0m in \u001b[0;36mlaunch_website\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tunnels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'streamlit run --server.port 80 app.py >/dev/null'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    399\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Your account is limited to 1 simultaneous ngrok agent sessions.\\nYou can run multiple simultaneous tunnels from a single agent session by defining the tunnels in your agent configuration file and starting them with the command `ngrok start --all`.\\nRead more about the agent configuration file: https://ngrok.com/docs/secure-tunnels/ngrok-agent/reference/config\\nYou can view your current agent sessions in the dashboard:\\nhttps://dashboard.ngrok.com/tunnels/agents\\r\\n\\r\\nERR_NGROK_108\\r\\n."
          ]
        }
      ],
      "source": [
        "launch_website()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz5YLZRc4M5Q"
      },
      "source": [
        "### **Checkboxes in Streamlit**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bms5zCX248U6"
      },
      "source": [
        "To add another level of interactivity to Streamlit, we can add checkboxes that the user can interact with. To do this, we can set a variable like so to a Streamlit checkbox element:\n",
        "```\n",
        "show_full_light_curve = st.checkbox(\"Show Full Light Curve\", value=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjPT88Xm76wl"
      },
      "source": [
        "Using this value, we can set it so that the graph is only displayed when the user selects it with a checkbox\n",
        "```\n",
        "if show_full_light_curve:\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(y=X_train[index], mode='lines', name='Light Curve'))\n",
        "    fig.add_shape(type=\"rect\",\n",
        "                  x0=t_0, y0=min(X_train[index])-5,\n",
        "                  x1=t_0+period, y1=max(X_train[index])+5,\n",
        "                  line=dict(color=\"Red\"),\n",
        "                  fillcolor=\"LightPink\", opacity=0.5)\n",
        "    fig.update_layout(title=\"Box Covering One Period of Exoplanet Transit\",\n",
        "                      xaxis_title=\"Observation Point\",\n",
        "                      yaxis_title=\"Normalized Flux\",\n",
        "                      showlegend=True)\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LfR6tOu8ETq"
      },
      "source": [
        "**Let's try this out with our full light curve, selected light curve, and confusion matrix!**\n",
        "\n",
        "Your code should be structured similarly to this:\n",
        "```\n",
        "- Setup Code\n",
        "- Show Prediction\n",
        "- Checkboxes\n",
        "- If statements for each graph\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcHE3BH0BLQu"
      },
      "source": [
        "Tip: Try to work things step by step, starting with the light curves, and then the confusion matrix!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17ozgmiY8C8X",
        "outputId": "01adeff8-d3f4-4e60-f79e-3af21988702d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "model_path = 'Planet_Hunters_CNN.h5'\n",
        "################################################################################\n",
        "##########=           (Setup Code from Previous NBs)           =################\n",
        "################################################################################\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Load data\n",
        "df_train = pd.read_csv('exoTrain.csv')\n",
        "df_train['LABEL'] = df_train['LABEL'] - 1\n",
        "\n",
        "# Load the CNN model\n",
        "cnn_model = tf.keras.models.load_model('cnn')\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Apply preprocessing steps to the dataframe.\"\"\"\n",
        "    X = df.drop('LABEL', axis=1).values\n",
        "    y = df['LABEL'].values\n",
        "    # Fourier transform\n",
        "    X = np.abs(np.fft.fft(X, axis=1))\n",
        "    # Savitzky-Golay filter\n",
        "    X = savgol_filter(X, 21, 4, deriv=0, axis=1)\n",
        "    # Normalize\n",
        "    X = normalize(X)\n",
        "    # Robust scaling\n",
        "    scaler = RobustScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    # SMOTE\n",
        "    smote = SMOTE()\n",
        "    X, y = smote.fit_resample(X, y)\n",
        "    # Expand dimensions for CNN input\n",
        "    X_cnn = np.expand_dims(X, axis=2)\n",
        "    return X, X_cnn, y\n",
        "\n",
        "X_train, X_train_cnn, y_train = preprocess_data(df_train)\n",
        "\n",
        "def predict_cnn(index):\n",
        "    \"\"\"Make a prediction using the CNN model.\"\"\"\n",
        "    tensor = X_train_cnn[index].reshape(1, -1, 1)\n",
        "    output = cnn_model.predict(tensor)\n",
        "    return output.flatten()[0]\n",
        "\n",
        "st.title('Exoplanet Light Curve Visualization with CNN Predictions')\n",
        "\n",
        "################################################################################\n",
        "##########=                   YOUR CODE BELOW                  =################\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9xh9_PH_-Zu",
        "outputId": "8efddf3a-2ea3-4526-f336-afd20db5d2fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "#@title Instructor Solution\n",
        "\n",
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Load data\n",
        "df_train = pd.read_csv('exoTrain.csv')\n",
        "df_train['LABEL'] = df_train['LABEL'] - 1\n",
        "\n",
        "# Load the CNN model\n",
        "cnn_model = tf.keras.models.load_model('cnn')\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Apply preprocessing steps to the dataframe.\"\"\"\n",
        "    X = df.drop('LABEL', axis=1).values\n",
        "    y = df['LABEL'].values\n",
        "    # Fourier transform\n",
        "    X = np.abs(np.fft.fft(X, axis=1))\n",
        "    # Savitzky-Golay filter\n",
        "    X = savgol_filter(X, 21, 4, deriv=0, axis=1)\n",
        "    # Normalize\n",
        "    X = normalize(X)\n",
        "    # Robust scaling\n",
        "    scaler = RobustScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    # SMOTE\n",
        "    smote = SMOTE()\n",
        "    X, y = smote.fit_resample(X, y)\n",
        "    # Expand dimensions for CNN input\n",
        "    X_cnn = np.expand_dims(X, axis=2)\n",
        "    return X, X_cnn, y\n",
        "\n",
        "X_train, X_train_cnn, y_train = preprocess_data(df_train)\n",
        "\n",
        "def predict_cnn(index):\n",
        "    \"\"\"Make a prediction using the CNN model.\"\"\"\n",
        "    tensor = X_train_cnn[index].reshape(1, -1, 1)\n",
        "    output = cnn_model.predict(tensor)\n",
        "    return output.flatten()[0]\n",
        "\n",
        "st.title('Exoplanet Light Curve Visualization with CNN Predictions')\n",
        "\n",
        "# Slider for selecting the index of the light curve\n",
        "index = st.slider(\"Select Index for Light Curve\", min_value=0, max_value=len(X_train)-1, value=12, step=1)\n",
        "\n",
        "# Display CNN prediction results\n",
        "cnn_prediction = predict_cnn(index)\n",
        "st.write(f\"CNN Prediction (probability of being an exoplanet): {cnn_prediction:.4f}\")\n",
        "\n",
        "t_0 = st.slider(\"Start of Period (t_0)\", min_value=0, max_value=3197, value=430, step=1)\n",
        "period = st.slider(\"Length of Period\", min_value=0, max_value=3197, value=1184, step=1)\n",
        "\n",
        "# Checkbox widgets for graph selection\n",
        "show_full_light_curve = st.checkbox(\"Show Full Light Curve\", value=True)\n",
        "show_selected_period = st.checkbox(\"Show Selected Period\", value=True)\n",
        "show_confusion_matrix = st.checkbox(\"Show Confusion Matrix\", value=True)\n",
        "\n",
        "# Plotly graph for full light curve with rectangle highlighting the period\n",
        "if show_full_light_curve:\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(y=X_train[index], mode='lines', name='Light Curve'))\n",
        "    fig.add_shape(type=\"rect\",\n",
        "                  x0=t_0, y0=min(X_train[index])-5,\n",
        "                  x1=t_0+period, y1=max(X_train[index])+5,\n",
        "                  line=dict(color=\"Red\"),\n",
        "                  fillcolor=\"LightPink\", opacity=0.5)\n",
        "    fig.update_layout(title=\"Box Covering One Period of Exoplanet Transit\",\n",
        "                      xaxis_title=\"Observation Point\",\n",
        "                      yaxis_title=\"Normalized Flux\",\n",
        "                      showlegend=True)\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "# Plotly graph for just the selected period\n",
        "if show_selected_period:\n",
        "    fig_period = go.Figure()\n",
        "    fig_period.add_trace(go.Scatter(y=X_train[index, t_0: t_0+period], mode='lines', name='Selected Period'))\n",
        "    fig_period.update_layout(title=\"Plot of Just One Period\",\n",
        "                             xaxis_title=\"Observation Point\",\n",
        "                             yaxis_title=\"Normalized Flux\",\n",
        "                             showlegend=True)\n",
        "    st.plotly_chart(fig_period, use_container_width=True)\n",
        "\n",
        "# Compute and plot confusion matrices\n",
        "cnn_pred_labels = (cnn_model.predict(X_train_cnn).flatten() > 0.5).astype(int)\n",
        "cm_cnn = confusion_matrix(y_train, cnn_pred_labels)\n",
        "\n",
        "# Confusion Matrix for CNN\n",
        "if show_confusion_matrix:\n",
        "    fig_cm_cnn = ff.create_annotated_heatmap(z=cm_cnn, x=['Pred 0', 'Pred 1'], y=['True 0', 'True 1'],\n",
        "                                             colorscale='Viridis', showscale=True)\n",
        "    fig_cm_cnn.update_layout(title='Confusion Matrix for CNN')\n",
        "    st.plotly_chart(fig_cm_cnn, use_container_width=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6Nd6J_KAYuD",
        "outputId": "07ffc6a1-d85e-4d01-8afe-370062f4f516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click this link to try your web app:\n",
            "NgrokTunnel: \"https://12dd-35-185-1-182.ngrok-free.app\" -> \"http://localhost:80\"\n",
            "2024-07-27 21:47:08.235834: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-27 21:47:08.235926: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-27 21:47:08.239692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-27 21:47:09.967782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "launch_website()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}