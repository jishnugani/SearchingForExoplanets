# -*- coding: utf-8 -*-
"""PlanetHunters_Section3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zj-LtnofJIuh2lUWyJRUfMRBSqmhWNWH

<font color="#de3023"><h1><b>REMINDER MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>

# Classifying Exoplanets

In this notebook, we'll continue improving our models for exoplanet classification!

We'll be:
*   Preprocessing the Dataset similar to before
*   Implementing more modern and complex machine learning architectures to see which one performs best!

## Exoplanet Classification




Previously, we were able to visualize and augment the dataset from Kepler. Now that we better understand the data that we're working with, we can begin to dive into more complex architectures to classify exoplanet stars, and the difficulties faced when doing so.

<font color=darkorange>**Change Hardware Accelerator to GPU to train faster (Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU)**
"""

#@title Run this code to get started
!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTrain.csv'
!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTest.csv'

from urllib.request import urlretrieve
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.nn.functional as F

from sklearn.neural_network import MLPClassifier

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.cluster import KMeans
from sklearn import  metrics
from sklearn import tree
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from scipy.signal import savgol_filter
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score,ConfusionMatrixDisplay,precision_score,recall_score,f1_score
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, normalize

import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Activation, Dropout, Flatten, Dense
from keras import optimizers
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam, SGD
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv1D, Conv2D, MaxPooling2D, BatchNormalization, MaxPooling1D
from keras.losses import categorical_crossentropy
from keras.regularizers import l2
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.models import load_model

df_train = pd.read_csv('exoTrain.csv')
df_train['LABEL'] = df_train['LABEL'] -1
df_test = pd.read_csv('exoTest.csv')
df_test['LABEL'] = df_test['LABEL'] - 1

def plot_graphs(history, best):

  plt.figure(figsize=[10,4])
  # summarize history for accuracy
  plt.subplot(121)
  plt.plot(history.history['accuracy'])
  plt.plot(history.history['val_accuracy'])
  plt.title('model accuracy across training\n best accuracy of %.02f'%best[1])
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')

  # summarize history for loss
  plt.subplot(122)
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title('model loss across training\n best loss of %.02f'%best[0])
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.show()

def analyze_results(model, X_train, y_train, X_test, y_test):
    """
    Helper function to help interpret and model performance.

    Args:
    model: estimator instance
    X_train: {array-like, sparse matrix} of shape (n_samples, n_features)
    Input values for model training.
    y_train : array-like of shape (n_samples,)
    Target values for model training.
    X_test: {array-like, sparse matrix} of shape (n_samples, n_features)
    Input values for model testing.
    y_test : array-like of shape (n_samples,)
    Target values for model testing.

    Returns:
    None
    """
    print("-------------------------------------------")
    print("Model Results")
    print("")
    print("Training:")
    if type(model) == keras.src.engine.sequential.Sequential:
      train_predictions = model.predict(X_train)
      train_predictions = (train_predictions > 0.5)
      cm = confusion_matrix(y_train, train_predictions)
      labels = [0, 1]
      df_cm = pd.DataFrame(cm,index = labels,columns = labels)
      fig = plt.figure()
      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')
      #plt.yticks([1.25, 3.75], labels,va='center')
      plt.title('Confusion Matrix - Training Data')
      plt.ylabel('True label')
      plt.xlabel('Predicted label')
      plt.show()
    else:
      plt.close()
      ConfusionMatrixDisplay.from_estimator(model,X_train,y_train)
      plt.show()

    print("Testing:")
    if type(model) == keras.src.engine.sequential.Sequential:
      test_predictions = model.predict(X_test)
      test_predictions = (test_predictions > 0.5)
      cm = confusion_matrix(y_test, test_predictions)
      labels = [0, 1]
      df_cm = pd.DataFrame(cm,index = labels,columns = labels)
      fig = plt.figure()
      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')
      #plt.yticks([1.25, 3.75], labels,va='center')
      plt.title('Confusion Matrix - Test Data')
      plt.ylabel('True label')
      plt.xlabel('Predicted label')
      plt.show()
    else:
      ConfusionMatrixDisplay.from_estimator(model,X_test,y_test)

X_train = df_train.drop('LABEL', axis=1)
y_train = df_train['LABEL']
X_test = df_test.drop('LABEL', axis=1)
y_test = df_test['LABEL']

"""Remember that `df_train` and `df_test` are the Pandas data frames that store our training and test datapoints. Similar to before, we'll now augment the data before exploring more modern, complex machine learning architectures."""

#@title Run this code to preprocess data
# Helper functions that we can run for the three augmentation functions that will be used, but not explroed in depth

def smote(a,b):
    model = SMOTE()
    X,y = model.fit_resample(a, b)
    return X,y

def savgol(df1,df2):
    x = savgol_filter(df1,21,4,deriv=0)
    y = savgol_filter(df2,21,4,deriv=0)
    return x,y

def fourier(df1,df2):
    X_train = np.abs(np.fft.fft(df1, axis=1))
    X_test = np.abs(np.fft.fft(df2, axis=1))
    return X_train,X_test

def handle_nan(df):
    # Replace NaN values with the mean of the column
    col_mean = np.nanmean(df, axis=0)
    inds = np.where(np.isnan(df))
    df[inds] = np.take(col_mean, inds[1])
    return df

def norm(df1, df2):
    df1 = handle_nan(df1)
    df2 = handle_nan(df2)
    X_train = normalize(df1)
    X_test = normalize(df2)
    return X_train, X_test

def robust(df1, df2):
    df1 = handle_nan(df1)
    df2 = handle_nan(df2)
    scaler = RobustScaler()
    X_train = scaler.fit_transform(df1)
    X_test = scaler.transform(df2)
    return X_train, X_test

fourier_X_train, fourier_X_test = fourier(X_train, X_test)
savgol_X_train, savgol_X_test = savgol(fourier_X_train, fourier_X_test)
norm_X_train, norm_X_test = norm(savgol_X_train,savgol_X_test)
robust_X_train, robust_X_test = robust(norm_X_train, norm_X_test)
smote_X_train,smote_y_train = smote(robust_X_train, y_train)

# Here we're adding the generated, augmented data onto the testing data
# aug_X_train, new_X_test_data, aug_y_train, new_y_test_data = train_test_split(smote_X_train, smote_y_train, test_size=0.3)
# aug_X_test = np.concatenate((robust_X_test, new_X_test_data), axis=0)
# aug_y_test = np.concatenate((y_test, new_y_test_data), axis=0)

"""Awesome! Now we'll have access to the augmented dataset as `smote_X_train`, `robust_X_test`, `smote_y_train`, and `y_test`. Note that we only augmented our training data and kept the testing data only with pre-processing.

(For further exploration and model comparison based off different datasets, you can explore the code block from above to access the different versions of the augmented data. For instance, what happens if we use the raw data? Normalized data?)

## Milestone 1: MLP

Let's start with neural nets!

MLP stands for Multi-layer Perceptron, a specific kind of simple neural network. Thankfully, this is something that `sklearn` supports, and it's already imported as `MLPClassifier`.


![visual](https://s3.amazonaws.com/stackabuse/media/intro-to-neural-networks-scikit-learn-3.png)

#### Step 1: Create our model

Let's complete this by using an `MLPClassifier` model imported by the `sklearn` package. You can view the original documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). Let's create a model with:
1. One hidden layer with 10 units
2. random_state = 1
3. 300 max iterations
"""

# Create an MLP model (will train later)

model = None ### YOUR CODE HERE

#@title Instructor Solution
# Create an MLP model (will train later)

model = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(10))

"""Now, train your model using `smote_X_train` and `smote_y_train`, and analyze its accuracy and confusion matrix!

You have access to all the methods used in the previous notebook.
"""

### YOUR CODE HERE

#@title Instructor Solution
model.fit(smote_X_train, smote_y_train)

train_predictions = model.predict(smote_X_train)
test_predictions = model.predict(robust_X_test)
print(accuracy_score(smote_y_train, train_predictions))
print(accuracy_score(y_test, test_predictions))
analyze_results(model=model, X_train=smote_X_train, y_train=smote_y_train, X_test=robust_X_test, y_test=y_test)

"""**Discuss:** Were the results what you were expecting? Why or why not? How does it compare with past model results?

What might be any potential issues that we might run into?

**Hint:** What are some downsides to a traditional MLP? What would happen if we shifted the data left or right?

#### (Optional) Exercise

How does the model perform with different amount of layers and different amount of neurons within each layer? How does the same model perform when tested on the original dataset? (`X_train`, `X_test`, `y_train`, `y_test`) How does it perform when trained on the original dataset?
"""

#@title Instructor Solution
# Create an MLP model (will train later)

model = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(10, 10))

model.fit(X_train, y_train)

train_predictions = model.predict(X_train)
test_predictions = model.predict(X_test)
print(accuracy_score(y_train, train_predictions))
print(accuracy_score(y_test, test_predictions))
analyze_results(model=model, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)

"""## Milestone 2: Neural Networks (Tensorflow and Keras)

Now let's do what we did before, but using `tensorflow` and `keras`. These libraries will be crucial as they will allow us to create more complex models.

We'll start by creating a similar model using these new packages.

We'll be using a `Sequential` model in order to act as a "list of layers", which we will define to match our previous example. Later, we'll use it to build more complex, advanced models. More information can be found [here](https://keras.io/api/layers/).

1. Add a `Dense` layer with 10 hidden units and a ReLU activation function. This layer also requires an `input_shape` parameter. What should the input shape be?

2. Add a `Dense` layer with 1 hidden unit and a sigmoid activation function. (This will be our output layer)

**Discuss:** Why is there only 1 neuron in the final layer? Why do we have two layers?

### Instructor Solution  
<details><summary>click to reveal!</summary>

*   There is only 1 neuron in the final layer since we want only a singular output from the model - if the given data is from an exoplanet or not (binary output).
*   The first layer is a hidden layer that computes and extracts important information from the data, and the second layer is the final layer that is our output layer.
"""

# Create a model (will train later)

# First, we initialize our model
model = Sequential()

#############

### YOUR CODE HERE

#############

# we finalize the model by "compiling" it and defining some other hyperparameters
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

#@title Instructor Solution
# Create an MLP model (will train later)

# First, we initialize our model
model = Sequential()
# then we add a "Dense" (i.e. fully connected) layer
model.add(Dense(10, input_shape=(3197,), activation = "relu")) # for the first layer we specify the input dimensions
# we end by defining the output layer, which has the number of dimensions of the predictions we're making
model.add(Dense(1, activation='sigmoid'))
# we finalize the model by "compiling" it and defining some other hyperparameters
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

"""Now to check the details of your model, run the code block below."""

model.summary()

"""Now train and analyze your `model` like you did before! You'll need to specify these parameters to `fit`:

1. `batch_size` = 64
2. `epochs` = 20
3. `verbose` = 1
4. `validation_data` = (`robust_X_test`, `y_test`)
5. `shuffle` = True

Save the history of the model as it trains or "fits" the data.

Hint:



```
history = model.fit(### YOUR CODE HERE)
```
"""

### YOUR CODE HERE to train the model

####################

#@title Instructor Solution
# Train and analyze the model
batch_size = 64
epochs = 20
validation_data = (robust_X_test, y_test)
verbose = 1
shuffle = True

history = model.fit(smote_X_train, smote_y_train, batch_size=batch_size, epochs=epochs, verbose=verbose,
                            validation_data=validation_data, shuffle=shuffle)

"""Now we will see how to view the performance of the model as it trained over time!

In addition, we still want to be able to plot the confusion matrix of the model to check for performance and potential class biases. Enter code to analyze the model in the empty portion of the codeblock below.
"""

performance = model.evaluate(robust_X_test, y_test, batch_size=batch_size)
plot_graphs(history, performance)

##############

### YOUR CODE HERE

##############

#@title Instructor Solution

performance = model.evaluate(robust_X_test, y_test, batch_size=batch_size)
plot_graphs(history, performance)

##############
analyze_results(model=model, X_train=smote_X_train, y_train=smote_y_train, X_test=robust_X_test, y_test=y_test)
##############

"""#### (Optional) Exercise

How does the model perform with different amount of layers and different amount of neurons within each layer? How does the same model perform of the original dataset? (`X_train`, `X_test`, `y_train`, `y_test`) How does it perform when trained on the original dataset?
"""

### YOUR CODE HERE

"""## Milestone 3: Convolutional Neural Network (CNN)

One potential fault of our previous approach is memorizing the placement of specific patterns in the data. Although we were able to achieve great levels of accuracy, we might benefit from an architecture that can make decisions based on patterns no matter where they occur in the sample - for example, if we started measuring flux earlier or later!

This is something that CNNs excel at. Most CNN architectures are set up to work with two dimensional inputs such as images, so our approach will be a bit different in working with and creating a one-dimensional CNN. However, similar concepts apply as we'll be passing a filter across the each data point with respect to time.

[Here](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) is a link to learn more about convolutional neural nets, and [here's](https://poloclub.github.io/cnn-explainer/) an interactive demo to explore. Try talking through the image of a traditional CNN below!

![](https://miro.medium.com/max/3288/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)

First, we'll have to "reshape" our augmented data into a shape that can be fed into a 1-dimensional CNN. We've reshaped the training data below - please **reshape the testing data, too.**

Note: No new information is created, but just the way the information is structured. Because of this, we should have the same number of values present overall with no modifications to the values themselves.
"""

cnn_smote_X_train = np.expand_dims(smote_X_train, axis=2)
cnn_smote_y_train = smote_y_train

############

cnn_preprocess_X_test = None ### YOUR CODE HERE
cnn_preprocess_y_test = None ### YOUR CODE HERE

############

cnn_X_train = np.expand_dims(X_train, axis=2)
cnn_y_train = y_train

############

cnn_X_test = None ### YOUR CODE HERE
cnn_y_test = None ### YOUR CODE HERE

############

#@title Instructor Solution
cnn_smote_X_train = np.expand_dims(smote_X_train, axis=2)
cnn_smote_y_train = smote_y_train

############
cnn_preprocess_X_test = np.expand_dims(robust_X_test, axis=2)
cnn_preprocess_y_test = y_test
############

cnn_X_train = np.expand_dims(X_train, axis=2)
cnn_y_train = y_train

############
cnn_X_test = np.expand_dims(X_test, axis=2)
cnn_y_test = y_test
############

"""What are the new shapes of the data? The new data is stored in variables: `cnn_smote_X_train`, `cnn_preprocess_X_test`, `cnn_smote_y_train`, `cnn_preprocess_y_test`."""

### YOUR CODE HERE

#@title Instructor Solution

print(cnn_smote_X_train.shape)
print(cnn_preprocess_X_test.shape)
print(cnn_smote_y_train.shape)
print(cnn_preprocess_y_test.shape)

"""Awesome!

Now, we'll be using a `Sequential` model to build up our CNN. Here's a suggestion for an architecture to start with:

1. Add a `Conv1D` layer with 8 output filters, kernal size of 5, relu activation function, and padding = 'same'. This layer also requires an `input_shape` parameter. Does the defined input_shape make sense?

2. Add a `MaxPooling1D` layer with pool_size = 4, strides = 4, and padding = 'same'.

3. Add a `Conv1D` layer with 16 output filters, kernal size of 3, relu activation function, and padding = 'same'.

4. Add a `MaxPooling1D` layer with pool_size = 4, strides = 4, and padding = 'same'.

5. Add a `Flatten` layer.

6. Add a `Dense` layer with 1 hidden unit and a sigmoid activation function. (This will be our output layer)

Discuss: Why is there only 1 neuron in the final layer? Why do we have the same loss function and metrics as the network before if we're using two different architectures?

### Instructor Solution  
<details><summary>click to reveal!</summary>

*    There is only 1 neuron in the final layer since we want only a singular output from the model - if the given data is from an exoplanet or not (binary output).
*   Even thought we are using two different architectures, the goal is the same - to identify exoplanets. Thus, the output of both architectures should be similar (even if one out performs the other), and the loss function and metrics mainly rely on the output of the model to compute their values.
"""

# Create model

# First, we initialize our model
model = Sequential()
input_shape = [3197, 1]

#######TODO#########

### YOUR CODE HERE

####################

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

#@title Instructor Solution
# Create a CNN model (will train later)

# First, we initialize our model
model = Sequential()
input_shape = [3197, 1]

cnn_layer_1 = Conv1D(8, 5, activation='relu', input_shape=input_shape, padding='same')
cnn_layer_2 = MaxPooling1D(pool_size=4, strides=4, padding='same')
cnn_layer_3 = Conv1D(16, 3, activation='relu', padding='same')
cnn_layer_4 = MaxPooling1D(pool_size=4, strides=4, padding='same')
cnn_layer_5 = Flatten()
cnn_layer_6 = Dense(1, activation='sigmoid')

# then we add a "Dense" (i.e. fully connected) layer
model.add(cnn_layer_1) # for the first layer we specify the input dimensions
model.add(cnn_layer_2)
model.add(cnn_layer_3)
model.add(cnn_layer_4)
model.add(cnn_layer_5)
model.add(cnn_layer_6)
# we end by defining the output layer, which has the number of dimensions of the predictions we're making
# model.add(Dense(1, activation='sigmoid'))
# we finalize the model by "compiling" it and defining some other hyperparameters
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

"""Now train the model like we did before! (Make sure to use the newly formatted data instead. This will include `cnn_smote_X_train`, `cnn_preprocess_X_test`, `cnn_smote_y_train`, and `cnn_preprocess_y_test`)"""

# Train the Model

################

### YOUR CODE HERE

################

#@title Instructor Solution
# Train and analyze the model

# Train the model, see accuracies, and analyze the results

#######TODO#########

#training the model
batch_size = 64
epochs = 20
validation_data = (cnn_preprocess_X_test, cnn_preprocess_y_test)
verbose = 1
shuffle = True

history = model.fit(cnn_smote_X_train, cnn_smote_y_train, batch_size=batch_size, epochs=epochs, verbose=verbose,
                            validation_data=validation_data, shuffle=shuffle)

####################

"""Once again, let's analyze the model's performance over time and the final confusion matrices:"""

### YOUR CODE HERE

#@title Instructor Solution

performance = model.evaluate(cnn_preprocess_X_test, cnn_preprocess_y_test, batch_size=batch_size)
plot_graphs(history, performance)

##############
analyze_results(model=model, X_train=cnn_smote_X_train, y_train=cnn_smote_y_train, X_test=cnn_preprocess_X_test, y_test=cnn_preprocess_y_test)
##############

#@title Run this to save your CNN model!
from google.colab import drive
drive.mount('/content/gdrive')
save_path = "/content/gdrive/My Drive/cnn.zip"

import tensorflow as tf

tf.keras.models.save_model(model,'cnn')
import zipfile

import os
import zipfile

def zipdir(path, ziph):
    for root, dirs, files in os.walk(path):
        for file in files:
            ziph.write(os.path.join(root, file))


zipf = zipfile.ZipFile(save_path, 'w', zipfile.ZIP_DEFLATED)
zipdir('cnn', zipf)
zipf.close()

"""#### (Optional) Exercise

How does the model perform with different amount of layers and different amount of neurons within each layer? How does the same model perform of the original dataset? (`cnn_X_train`, `cnn_X_test`, `cnn_y_train`, `cnn_y_test`, these are the same original values but resized to work with the new architecture) How does it perform when trained on the original dataset?

**Hint:** Try out new layers like a "Dropout" layer. More complex architectures often lead to better results as well. Try increasing the number of channels in each convolution layer or adding more layers in general!
"""

### YOUR CODE HERE

"""## Milestone 4: Optional Exploration

Congratulations! You've learned to visually analyze and refine raw satellite data, and built a top-of the line model that accurately detects exoplanet stars vs. non-exoplanet stars. This is critical to exoplanet hunting because it allows planetary hunters to focus on studying the exoplanets we've discovered, and analyzing them for mass, habitability, etc.

Remember that in our original dataset, exoplanet stars accounted for less than 1 % of all samples collected. In notebooks 2 and 3, we used machine learning to automatically identify likely exoplanet stars, dramatically reducing the time and effort needed to find them!

This pipeline can be used to help aid the search of exoplanets for the incoming, raw data. It might even lead to new planetary discoveries as space exploration continues!

Of course, the more data, the better. This model and pipeline can be further improved with future iterations of new data and architectures. If you decide to go planet hunting, have fun on your new adventure!

**Optional**

Modify past model architectures or create new ones! See how they perform with the augmented dataset vs the original dataset! We can even explore different pre-processing techniques to help imporve the quality of our data! How will the new models, modifications, or pre-processing techniques compare to our past results? Which is the best solution?

**Hint**

If looking for inspiration, go to Tensorflow and check out different kinds of layers and activation functions! You can find the original documentation [here](https://keras.io/api/layers/).
"""

### YOUR CODE HERE - happy planet hunting!