{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"#de3023\"><h1><b>REMINDER MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>\n"
      ],
      "metadata": {
        "id": "pPu6lFdWemiG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0ygrZd3dmu5"
      },
      "source": [
        "# Classifying Exoplanets\n",
        "\n",
        "In this notebook, we'll continue improving our models for exoplanet classification!\n",
        "\n",
        "We'll be:\n",
        "*   Preprocessing the Dataset similar to before\n",
        "*   Implementing more modern and complex machine learning architectures to see which one performs best!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irPDgkzsdguU"
      },
      "source": [
        "## Exoplanet Classification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Previously, we were able to visualize and augment the dataset from Kepler. Now that we better understand the data that we're working with, we can begin to dive into more complex architectures to classify exoplanet stars, and the difficulties faced when doing so."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=darkorange>**Change Hardware Accelerator to GPU to train faster (Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU)**\n"
      ],
      "metadata": {
        "id": "Gd53z1-PfLTx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjzYKqcweS2F",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6909bbf-0506-4333-ba84-6b686b4e52fe"
      },
      "source": [
        "#@title Run this code to get started\n",
        "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTrain.csv'\n",
        "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTest.csv'\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import  metrics\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score,ConfusionMatrixDisplay,precision_score,recall_score,f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, normalize\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv1D, Conv2D, MaxPooling2D, BatchNormalization, MaxPooling1D\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "\n",
        "df_train = pd.read_csv('exoTrain.csv')\n",
        "df_train['LABEL'] = df_train['LABEL'] -1\n",
        "df_test = pd.read_csv('exoTest.csv')\n",
        "df_test['LABEL'] = df_test['LABEL'] - 1\n",
        "\n",
        "def plot_graphs(history, best):\n",
        "\n",
        "  plt.figure(figsize=[10,4])\n",
        "  # summarize history for accuracy\n",
        "  plt.subplot(121)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy across training\\n best accuracy of %.02f'%best[1])\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "  # summarize history for loss\n",
        "  plt.subplot(122)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss across training\\n best loss of %.02f'%best[0])\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def analyze_results(model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Helper function to help interpret and model performance.\n",
        "\n",
        "    Args:\n",
        "    model: estimator instance\n",
        "    X_train: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "    Input values for model training.\n",
        "    y_train : array-like of shape (n_samples,)\n",
        "    Target values for model training.\n",
        "    X_test: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "    Input values for model testing.\n",
        "    y_test : array-like of shape (n_samples,)\n",
        "    Target values for model testing.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    print(\"-------------------------------------------\")\n",
        "    print(\"Model Results\")\n",
        "    print(\"\")\n",
        "    print(\"Training:\")\n",
        "    if type(model) == keras.src.engine.sequential.Sequential:\n",
        "      train_predictions = model.predict(X_train)\n",
        "      train_predictions = (train_predictions > 0.5)\n",
        "      cm = confusion_matrix(y_train, train_predictions)\n",
        "      labels = [0, 1]\n",
        "      df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
        "      fig = plt.figure()\n",
        "      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "      #plt.yticks([1.25, 3.75], labels,va='center')\n",
        "      plt.title('Confusion Matrix - Training Data')\n",
        "      plt.ylabel('True label')\n",
        "      plt.xlabel('Predicted label')\n",
        "      plt.show()\n",
        "    else:\n",
        "      plt.close()\n",
        "      ConfusionMatrixDisplay.from_estimator(model,X_train,y_train)\n",
        "      plt.show()\n",
        "\n",
        "    print(\"Testing:\")\n",
        "    if type(model) == keras.src.engine.sequential.Sequential:\n",
        "      test_predictions = model.predict(X_test)\n",
        "      test_predictions = (test_predictions > 0.5)\n",
        "      cm = confusion_matrix(y_test, test_predictions)\n",
        "      labels = [0, 1]\n",
        "      df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
        "      fig = plt.figure()\n",
        "      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "      #plt.yticks([1.25, 3.75], labels,va='center')\n",
        "      plt.title('Confusion Matrix - Test Data')\n",
        "      plt.ylabel('True label')\n",
        "      plt.xlabel('Predicted label')\n",
        "      plt.show()\n",
        "    else:\n",
        "      ConfusionMatrixDisplay.from_estimator(model,X_test,y_test)\n",
        "\n",
        "X_train = df_train.drop('LABEL', axis=1)\n",
        "y_train = df_train['LABEL']\n",
        "X_test = df_test.drop('LABEL', axis=1)\n",
        "y_test = df_test['LABEL']"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exoTrain.csv.3      100%[===================>] 250.08M  22.2MB/s    in 12s     \n",
            "exoTest.csv.3       100%[===================>]  27.57M  11.9MB/s    in 2.3s    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAc67kxQftEV"
      },
      "source": [
        "Remember that `df_train` and `df_test` are the Pandas data frames that store our training and test datapoints. Similar to before, we'll now augment the data before exploring more modern, complex machine learning architectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yRtFD1pYP7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "f49369bf-b170-4b7b-ec34-564acf3a9791"
      },
      "source": [
        "#@title Run this code to preprocess data\n",
        "# Helper functions that we can run for the three augmentation functions that will be used, but not explroed in depth\n",
        "\n",
        "def smote(a,b):\n",
        "    model = SMOTE()\n",
        "    X,y = model.fit_resample(a, b)\n",
        "    return X,y\n",
        "\n",
        "def savgol(df1,df2):\n",
        "    x = savgol_filter(df1,21,4,deriv=0)\n",
        "    y = savgol_filter(df2,21,4,deriv=0)\n",
        "    return x,y\n",
        "\n",
        "def fourier(df1,df2):\n",
        "    X_train = np.abs(np.fft.fft(df1, axis=1))\n",
        "    X_test = np.abs(np.fft.fft(df2, axis=1))\n",
        "    return X_train,X_test\n",
        "\n",
        "def norm(df1,df2):\n",
        "    X_train = normalize(df1)\n",
        "    X_test = normalize(df2)\n",
        "    return X_train,X_test\n",
        "\n",
        "def robust(df1,df2):\n",
        "    scaler = RobustScaler()\n",
        "    X_train = scaler.fit_transform(df1)\n",
        "    X_test = scaler.transform(df2)\n",
        "    return X_train,X_test\n",
        "\n",
        "fourier_X_train, fourier_X_test = fourier(X_train, X_test)\n",
        "savgol_X_train, savgol_X_test = savgol(fourier_X_train, fourier_X_test)\n",
        "norm_X_train, norm_X_test = norm(savgol_X_train,savgol_X_test)\n",
        "robust_X_train, robust_X_test = robust(norm_X_train, norm_X_test)\n",
        "smote_X_train,smote_y_train = smote(robust_X_train, y_train)\n",
        "\n",
        "# Here we're adding the generated, augmented data onto the testing data\n",
        "# aug_X_train, new_X_test_data, aug_y_train, new_y_test_data = train_test_split(smote_X_train, smote_y_train, test_size=0.3)\n",
        "# aug_X_test = np.concatenate((robust_X_test, new_X_test_data), axis=0)\n",
        "# aug_y_test = np.concatenate((y_test, new_y_test_data), axis=0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input contains NaN.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e8d4a7a8cc3f>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mfourier_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfourier_X_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfourier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0msavgol_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavgol_X_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msavgol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfourier_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfourier_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mnorm_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_X_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavgol_X_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msavgol_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mrobust_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrobust_X_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrobust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0msmote_X_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmote_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrobust_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e8d4a7a8cc3f>\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(df1, df2)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     )\n\u001b[1;32m    213\u001b[0m                 ):\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1839\u001b[0m         \u001b[0msparse_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1841\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1842\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    958\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             )\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRQPCRgpZVpW"
      },
      "source": [
        "Awesome! Now we'll have access to the augmented dataset as `smote_X_train`, `robust_X_test`, `smote_y_train`, and `y_test`. Note that we only augmented our training data and kept the testing data only with pre-processing.\n",
        "\n",
        "(For further exploration and model comparison based off different datasets, you can explore the code block from above to access the different versions of the augmented data. For instance, what happens if we use the raw data? Normalized data?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw7FwS0_W7Bx"
      },
      "source": [
        "## Milestone 1: MLP\n",
        "\n",
        "Let's start with neural nets!\n",
        "\n",
        "MLP stands for Multi-layer Perceptron, a specific kind of simple neural network. Thankfully, this is something that `sklearn` supports, and it's already imported as `MLPClassifier`.\n",
        "\n",
        "\n",
        "![visual](https://s3.amazonaws.com/stackabuse/media/intro-to-neural-networks-scikit-learn-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmbSCBCH2I9p"
      },
      "source": [
        "#### Step 1: Create our model\n",
        "\n",
        "Let's complete this by using an `MLPClassifier` model imported by the `sklearn` package. You can view the original documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). Let's create a model with:\n",
        "1. One hidden layer with 10 units\n",
        "2. random_state = 1\n",
        "3. 300 max iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rVs2i_C_F1o"
      },
      "source": [
        "# Create an MLP model (will train later)\n",
        "\n",
        "model = None ### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBW-7S3mHVyA"
      },
      "source": [
        "#@title Instructor Solution\n",
        "# Create an MLP model (will train later)\n",
        "\n",
        "model = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXj7fzfkXuQ9"
      },
      "source": [
        "Now, train your model using `smote_X_train` and `smote_y_train`, and analyze its accuracy and confusion matrix!\n",
        "\n",
        "You have access to all the methods used in the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5okHo29ZXt13"
      },
      "source": [
        "### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nFtWOLkXx5N"
      },
      "source": [
        "#@title Instructor Solution\n",
        "model.fit(smote_X_train, smote_y_train)\n",
        "\n",
        "train_predictions = model.predict(smote_X_train)\n",
        "test_predictions = model.predict(robust_X_test)\n",
        "print(accuracy_score(smote_y_train, train_predictions))\n",
        "print(accuracy_score(y_test, test_predictions))\n",
        "analyze_results(model=model, X_train=smote_X_train, y_train=smote_y_train, X_test=robust_X_test, y_test=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdW3vidfZQxg"
      },
      "source": [
        "**Discuss:** Were the results what you were expecting? Why or why not? How does it compare with past model results?\n",
        "\n",
        "What might be any potential issues that we might run into?\n",
        "\n",
        "**Hint:** What are some downsides to a traditional MLP? What would happen if we shifted the data left or right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cYzv7-ZvNMb"
      },
      "source": [
        "#### (Optional) Exercise\n",
        "\n",
        "How does the model perform with different amount of layers and different amount of neurons within each layer? How does the same model perform when tested on the original dataset? (`X_train`, `X_test`, `y_train`, `y_test`) How does it perform when trained on the original dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQvSf0VOvRxe"
      },
      "source": [
        "#@title Instructor Solution\n",
        "# Create an MLP model (will train later)\n",
        "\n",
        "model = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(10, 10))\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "train_predictions = model.predict(X_train)\n",
        "test_predictions = model.predict(X_test)\n",
        "print(accuracy_score(y_train, train_predictions))\n",
        "print(accuracy_score(y_test, test_predictions))\n",
        "analyze_results(model=model, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRF34weGoClK"
      },
      "source": [
        "## Milestone 2: Neural Networks (Tensorflow and Keras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbCq5LpQgVkF"
      },
      "source": [
        "Now let's do what we did before, but using `tensorflow` and `keras`. These libraries will be crucial as they will allow us to create more complex models.\n",
        "\n",
        "We'll start by creating a similar model using these new packages.\n",
        "\n",
        "We'll be using a `Sequential` model in order to act as a \"list of layers\", which we will define to match our previous example. Later, we'll use it to build more complex, advanced models. More information can be found [here](https://keras.io/api/layers/).\n",
        "\n",
        "1. Add a `Dense` layer with 10 hidden units and a ReLU activation function. This layer also requires an `input_shape` parameter. What should the input shape be?\n",
        "\n",
        "2. Add a `Dense` layer with 1 hidden unit and a sigmoid activation function. (This will be our output layer)\n",
        "\n",
        "**Discuss:** Why is there only 1 neuron in the final layer? Why do we have two layers?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution  \n",
        "<details><summary>click to reveal!</summary>\n",
        "\n",
        "*   There is only 1 neuron in the final layer since we want only a singular output from the model - if the given data is from an exoplanet or not (binary output).\n",
        "*   The first layer is a hidden layer that computes and extracts important information from the data, and the second layer is the final layer that is our output layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "qTx7FhyCntjG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shZOUCAbBJ0M"
      },
      "source": [
        "# Create a model (will train later)\n",
        "\n",
        "# First, we initialize our model\n",
        "model = Sequential()\n",
        "\n",
        "#############\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "#############\n",
        "\n",
        "# we finalize the model by \"compiling\" it and defining some other hyperparameters\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXvobnHRJg7L"
      },
      "source": [
        "#@title Instructor Solution\n",
        "# Create an MLP model (will train later)\n",
        "\n",
        "# First, we initialize our model\n",
        "model = Sequential()\n",
        "# then we add a \"Dense\" (i.e. fully connected) layer\n",
        "model.add(Dense(10, input_shape=(3197,), activation = \"relu\")) # for the first layer we specify the input dimensions\n",
        "# we end by defining the output layer, which has the number of dimensions of the predictions we're making\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# we finalize the model by \"compiling\" it and defining some other hyperparameters\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sriqCvylFPSE"
      },
      "source": [
        "Now to check the details of your model, run the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUdk-_M3FTqA"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QaGfpePBSKS"
      },
      "source": [
        "Now train and analyze your `model` like you did before! You'll need to specify these parameters to `fit`:\n",
        "\n",
        "1. `batch_size` = 64\n",
        "2. `epochs` = 20\n",
        "3. `verbose` = 1\n",
        "4. `validation_data` = (`robust_X_test`, `y_test`)\n",
        "5. `shuffle` = True\n",
        "\n",
        "Save the history of the model as it trains or \"fits\" the data.\n",
        "\n",
        "Hint:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "history = model.fit(### YOUR CODE HERE)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9Y1kOWtnOTv"
      },
      "source": [
        "### YOUR CODE HERE to train the model\n",
        "\n",
        "####################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cu7wdxOBc3z",
        "cellView": "form"
      },
      "source": [
        "#@title Instructor Solution\n",
        "# Train and analyze the model\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "validation_data = (robust_X_test, y_test)\n",
        "verbose = 1\n",
        "shuffle = True\n",
        "\n",
        "history = model.fit(smote_X_train, smote_y_train, batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
        "                            validation_data=validation_data, shuffle=shuffle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIBRY5RnbrMY"
      },
      "source": [
        "Now we will see how to view the performance of the model as it trained over time!\n",
        "\n",
        "In addition, we still want to be able to plot the confusion matrix of the model to check for performance and potential class biases. Enter code to analyze the model in the empty portion of the codeblock below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDly6NJObeiP"
      },
      "source": [
        "performance = model.evaluate(robust_X_test, y_test, batch_size=batch_size)\n",
        "plot_graphs(history, performance)\n",
        "\n",
        "##############\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "##############"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV8K0HxgKQTt",
        "cellView": "form"
      },
      "source": [
        "#@title Instructor Solution\n",
        "\n",
        "performance = model.evaluate(robust_X_test, y_test, batch_size=batch_size)\n",
        "plot_graphs(history, performance)\n",
        "\n",
        "##############\n",
        "analyze_results(model=model, X_train=smote_X_train, y_train=smote_y_train, X_test=robust_X_test, y_test=y_test)\n",
        "##############"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbrQWbPYhfwZ"
      },
      "source": [
        "#### (Optional) Exercise\n",
        "\n",
        "How does the model perform with different amount of layers and different amount of neurons within each layer? How does the same model perform of the original dataset? (`X_train`, `X_test`, `y_train`, `y_test`) How does it perform when trained on the original dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlXvkwTEhpNw"
      },
      "source": [
        "### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA_rRs1jTjDw"
      },
      "source": [
        "## Milestone 3: Convolutional Neural Network (CNN)\n",
        "\n",
        "One potential fault of our previous approach is memorizing the placement of specific patterns in the data. Although we were able to achieve great levels of accuracy, we might benefit from an architecture that can make decisions based on patterns no matter where they occur in the sample - for example, if we started measuring flux earlier or later!\n",
        "\n",
        "This is something that CNNs excel at. Most CNN architectures are set up to work with two dimensional inputs such as images, so our approach will be a bit different in working with and creating a one-dimensional CNN. However, similar concepts apply as we'll be passing a filter across the each data point with respect to time.\n",
        "\n",
        "[Here](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) is a link to learn more about convolutional neural nets, and [here's](https://poloclub.github.io/cnn-explainer/) an interactive demo to explore. Try talking through the image of a traditional CNN below!\n",
        "\n",
        "![](https://miro.medium.com/max/3288/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnQlAZ043qOo"
      },
      "source": [
        "First, we'll have to \"reshape\" our augmented data into a shape that can be fed into a 1-dimensional CNN. We've reshaped the training data below - please **reshape the testing data, too.**\n",
        "\n",
        "Note: No new information is created, but just the way the information is structured. Because of this, we should have the same number of values present overall with no modifications to the values themselves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMWiJnl735CQ"
      },
      "source": [
        "cnn_smote_X_train = np.expand_dims(smote_X_train, axis=2)\n",
        "cnn_smote_y_train = smote_y_train\n",
        "\n",
        "############\n",
        "\n",
        "cnn_preprocess_X_test = None ### YOUR CODE HERE\n",
        "cnn_preprocess_y_test = None ### YOUR CODE HERE\n",
        "\n",
        "############\n",
        "\n",
        "cnn_X_train = np.expand_dims(X_train, axis=2)\n",
        "cnn_y_train = y_train\n",
        "\n",
        "############\n",
        "\n",
        "cnn_X_test = None ### YOUR CODE HERE\n",
        "cnn_y_test = None ### YOUR CODE HERE\n",
        "\n",
        "############"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogS5RYSsz5qN",
        "cellView": "form"
      },
      "source": [
        "#@title Instructor Solution\n",
        "cnn_smote_X_train = np.expand_dims(smote_X_train, axis=2)\n",
        "cnn_smote_y_train = smote_y_train\n",
        "\n",
        "############\n",
        "cnn_preprocess_X_test = np.expand_dims(robust_X_test, axis=2)\n",
        "cnn_preprocess_y_test = y_test\n",
        "############\n",
        "\n",
        "cnn_X_train = np.expand_dims(X_train, axis=2)\n",
        "cnn_y_train = y_train\n",
        "\n",
        "############\n",
        "cnn_X_test = np.expand_dims(X_test, axis=2)\n",
        "cnn_y_test = y_test\n",
        "############"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtHMC1fJWsrM"
      },
      "source": [
        "What are the new shapes of the data? The new data is stored in variables: `cnn_smote_X_train`, `cnn_preprocess_X_test`, `cnn_smote_y_train`, `cnn_preprocess_y_test`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcti4YgIXdN4"
      },
      "source": [
        "### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfWG_Vd7XgTG",
        "cellView": "form"
      },
      "source": [
        "#@title Instructor Solution\n",
        "\n",
        "print(cnn_smote_X_train.shape)\n",
        "print(cnn_preprocess_X_test.shape)\n",
        "print(cnn_smote_y_train.shape)\n",
        "print(cnn_preprocess_y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5bXRoaCTIj"
      },
      "source": [
        "Awesome!\n",
        "\n",
        "Now, we'll be using a `Sequential` model to build up our CNN. Here's a suggestion for an architecture to start with:\n",
        "\n",
        "1. Add a `Conv1D` layer with 8 output filters, kernal size of 5, relu activation function, and padding = 'same'. This layer also requires an `input_shape` parameter. Does the defined input_shape make sense?\n",
        "\n",
        "2. Add a `MaxPooling1D` layer with pool_size = 4, strides = 4, and padding = 'same'.\n",
        "\n",
        "3. Add a `Conv1D` layer with 16 output filters, kernal size of 3, relu activation function, and padding = 'same'.\n",
        "\n",
        "4. Add a `MaxPooling1D` layer with pool_size = 4, strides = 4, and padding = 'same'.\n",
        "\n",
        "5. Add a `Flatten` layer.\n",
        "\n",
        "6. Add a `Dense` layer with 1 hidden unit and a sigmoid activation function. (This will be our output layer)\n",
        "\n",
        "Discuss: Why is there only 1 neuron in the final layer? Why do we have the same loss function and metrics as the network before if we're using two different architectures?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution  \n",
        "<details><summary>click to reveal!</summary>\n",
        "\n",
        "*    There is only 1 neuron in the final layer since we want only a singular output from the model - if the given data is from an exoplanet or not (binary output).\n",
        "*   Even thought we are using two different architectures, the goal is the same - to identify exoplanets. Thus, the output of both architectures should be similar (even if one out performs the other), and the loss function and metrics mainly rely on the output of the model to compute their values."
      ],
      "metadata": {
        "id": "y8OAcJ_CovqG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmW5k1otDu79"
      },
      "source": [
        "# Create model\n",
        "\n",
        "# First, we initialize our model\n",
        "model = Sequential()\n",
        "input_shape = [3197, 1]\n",
        "\n",
        "#######TODO#########\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "####################\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5S5wXQMTnEb",
        "cellView": "form"
      },
      "source": [
        "#@title Instructor Solution\n",
        "# Create a CNN model (will train later)\n",
        "\n",
        "# First, we initialize our model\n",
        "model = Sequential()\n",
        "input_shape = [3197, 1]\n",
        "\n",
        "cnn_layer_1 = Conv1D(8, 5, activation='relu', input_shape=input_shape, padding='same')\n",
        "cnn_layer_2 = MaxPooling1D(pool_size=4, strides=4, padding='same')\n",
        "cnn_layer_3 = Conv1D(16, 3, activation='relu', padding='same')\n",
        "cnn_layer_4 = MaxPooling1D(pool_size=4, strides=4, padding='same')\n",
        "cnn_layer_5 = Flatten()\n",
        "cnn_layer_6 = Dense(1, activation='sigmoid')\n",
        "\n",
        "# then we add a \"Dense\" (i.e. fully connected) layer\n",
        "model.add(cnn_layer_1) # for the first layer we specify the input dimensions\n",
        "model.add(cnn_layer_2)\n",
        "model.add(cnn_layer_3)\n",
        "model.add(cnn_layer_4)\n",
        "model.add(cnn_layer_5)\n",
        "model.add(cnn_layer_6)\n",
        "# we end by defining the output layer, which has the number of dimensions of the predictions we're making\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "# we finalize the model by \"compiling\" it and defining some other hyperparameters\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbDT84eyDGHY"
      },
      "source": [
        "Now train the model like we did before! (Make sure to use the newly formatted data instead. This will include `cnn_smote_X_train`, `cnn_preprocess_X_test`, `cnn_smote_y_train`, and `cnn_preprocess_y_test`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqX49x9pDvql"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "################\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVUCehR5TvLB",
        "cellView": "form"
      },
      "source": [
        "#@title Instructor Solution\n",
        "# Train and analyze the model\n",
        "\n",
        "# Train the model, see accuracies, and analyze the results\n",
        "\n",
        "#######TODO#########\n",
        "\n",
        "#training the model\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "validation_data = (cnn_preprocess_X_test, cnn_preprocess_y_test)\n",
        "verbose = 1\n",
        "shuffle = True\n",
        "\n",
        "history = model.fit(cnn_smote_X_train, cnn_smote_y_train, batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
        "                            validation_data=validation_data, shuffle=shuffle)\n",
        "\n",
        "####################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StKM_5W6uOYZ"
      },
      "source": [
        "Once again, let's analyze the model's performance over time and the final confusion matrices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C4uq3IUEHbo"
      },
      "source": [
        "### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPik55GguYUT",
        "cellView": "form"
      },
      "source": [
        "#@title Instructor Solution\n",
        "\n",
        "performance = model.evaluate(cnn_preprocess_X_test, cnn_preprocess_y_test, batch_size=batch_size)\n",
        "plot_graphs(history, performance)\n",
        "\n",
        "##############\n",
        "analyze_results(model=model, X_train=cnn_smote_X_train, y_train=cnn_smote_y_train, X_test=cnn_preprocess_X_test, y_test=cnn_preprocess_y_test)\n",
        "##############"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this to save your CNN model!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "save_path = \"/content/gdrive/My Drive/cnn.zip\"\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.models.save_model(model,'cnn')\n",
        "import zipfile\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def zipdir(path, ziph):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            ziph.write(os.path.join(root, file))\n",
        "\n",
        "\n",
        "zipf = zipfile.ZipFile(save_path, 'w', zipfile.ZIP_DEFLATED)\n",
        "zipdir('cnn', zipf)\n",
        "zipf.close()"
      ],
      "metadata": {
        "id": "3Aa4Y5rvlaGm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92tn05OBh0S2"
      },
      "source": [
        "#### (Optional) Exercise\n",
        "\n",
        "How does the model perform with different amount of layers and different amount of neurons within each layer? How does the same model perform of the original dataset? (`cnn_X_train`, `cnn_X_test`, `cnn_y_train`, `cnn_y_test`, these are the same original values but resized to work with the new architecture) How does it perform when trained on the original dataset?\n",
        "\n",
        "**Hint:** Try out new layers like a \"Dropout\" layer. More complex architectures often lead to better results as well. Try increasing the number of channels in each convolution layer or adding more layers in general!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVu5mBrwh1mA"
      },
      "source": [
        "### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzWrP7kYgdVo"
      },
      "source": [
        "## Milestone 4: Optional Exploration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svt0LBR34_WT"
      },
      "source": [
        "Congratulations! You've learned to visually analyze and refine raw satellite data, and built a top-of the line model that accurately detects exoplanet stars vs. non-exoplanet stars. This is critical to exoplanet hunting because it allows planetary hunters to focus on studying the exoplanets we've discovered, and analyzing them for mass, habitability, etc.\n",
        "\n",
        "Remember that in our original dataset, exoplanet stars accounted for less than 1 % of all samples collected. In notebooks 2 and 3, we used machine learning to automatically identify likely exoplanet stars, dramatically reducing the time and effort needed to find them!\n",
        "\n",
        "This pipeline can be used to help aid the search of exoplanets for the incoming, raw data. It might even lead to new planetary discoveries as space exploration continues!\n",
        "\n",
        "Of course, the more data, the better. This model and pipeline can be further improved with future iterations of new data and architectures. If you decide to go planet hunting, have fun on your new adventure!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XlouEr3ggRN"
      },
      "source": [
        "**Optional**\n",
        "\n",
        "Modify past model architectures or create new ones! See how they perform with the augmented dataset vs the original dataset! We can even explore different pre-processing techniques to help imporve the quality of our data! How will the new models, modifications, or pre-processing techniques compare to our past results? Which is the best solution?\n",
        "\n",
        "**Hint**\n",
        "\n",
        "If looking for inspiration, go to Tensorflow and check out different kinds of layers and activation functions! You can find the original documentation [here](https://keras.io/api/layers/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWfGsXkpw00u"
      },
      "source": [
        "### YOUR CODE HERE - happy planet hunting!"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}